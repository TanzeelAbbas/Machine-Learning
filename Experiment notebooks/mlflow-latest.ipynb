{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"42084110-295b-493a-9b3e-5d8d29ff78b3","showTitle":false,"title":""}},"source":["# LLM RAG Evaluation with MLflow using llama2-as-judge Example Notebook\n","\n","In this notebook, we will demonstrate how to evaluate various a RAG system with MLflow. We will use llama2-70b as the judge model, via a Databricks serving endpoint."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T05:33:00.249266Z","iopub.status.busy":"2024-02-29T05:33:00.248987Z","iopub.status.idle":"2024-02-29T05:33:00.259605Z","shell.execute_reply":"2024-02-29T05:33:00.258808Z","shell.execute_reply.started":"2024-02-29T05:33:00.249241Z"},"trusted":true},"outputs":[],"source":["import os"]},{"cell_type":"markdown","metadata":{},"source":["We need to set our OpenAI API key.\n","\n","In order to set your private key safely, please be sure to either export your key through a command-line terminal for your current instance, or, for a permanent addition to all user-based sessions, configure your favored environment management configuration file (i.e., .bashrc, .zshrc) to have the following entry:\n","\n","`OPENAI_API_KEY=<your openai API key>`"]},{"cell_type":"code","execution_count":2,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"bec25067-224d-4ee8-9b5d-0beeb6cde684","showTitle":false,"title":""}},"outputs":[],"source":["os.environ[\"DATABRICKS_HOST\"] = \"REDACTED\"\n","os.environ[\"DATABRICKS_TOKEN\"] = \"REDACTED\""]},{"cell_type":"markdown","metadata":{},"source":["Set the deployment target to \"databricks\" for use with Databricks served models."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from mlflow.deployments import set_deployments_target\n","\n","set_deployments_target(\"databricks\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"273d1345-95d7-435a-a7b6-a5f3dbb3f073","showTitle":false,"title":""}},"source":["## Create a RAG system\n","\n","Use Langchain and Chroma to create a RAG system that answers questions based on the MLflow documentation."]},{"cell_type":"code","execution_count":2,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"2c28d0ad-f469-46ab-a2b4-c5e8db50a729","showTitle":false,"title":""},"execution":{"iopub.execute_input":"2024-02-29T05:39:49.376270Z","iopub.status.busy":"2024-02-29T05:39:49.375398Z","iopub.status.idle":"2024-02-29T05:39:49.384123Z","shell.execute_reply":"2024-02-29T05:39:49.383084Z","shell.execute_reply.started":"2024-02-29T05:39:49.376235Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# !pip install langchain\n","# !pip install transformers\n","# !pip install chromadb\n","# !pip install sentence_transformers\n","# !pip install accelerate\n","# !pip install mlflow\n","# !pip install bitsandbytes\n","# !pip install rank_bm25 > /dev/null\n","import os, glob, textwrap, time\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.document_loaders import DirectoryLoader\n","from langchain.document_loaders import TextLoader\n","from langchain.chains import RetrievalQA\n","from langchain.embeddings import HuggingFaceBgeEmbeddings\n","from transformers import pipeline\n","from langchain.llms import HuggingFacePipeline\n","from langchain.vectorstores.chroma import Chroma\n","from langchain.chains import ConversationalRetrievalChain\n","from langchain.retrievers import BM25Retriever, EnsembleRetriever\n","from langchain import PromptTemplate\n","from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n","from langchain.memory import ConversationBufferMemory\n","from langchain.chains.question_answering import load_qa_chain\n","import pandas as pd\n","import mlflow"]},{"cell_type":"code","execution_count":3,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"83a7e77e-6717-472a-86dc-02e2c356ddef","showTitle":false,"title":""},"execution":{"iopub.execute_input":"2024-02-29T05:39:55.879218Z","iopub.status.busy":"2024-02-29T05:39:55.878481Z","iopub.status.idle":"2024-02-29T05:39:55.920990Z","shell.execute_reply":"2024-02-29T05:39:55.920148Z","shell.execute_reply.started":"2024-02-29T05:39:55.879185Z"},"trusted":true},"outputs":[],"source":["def loadSplitDocuments(file_path, chunk_size, chunk_overlap):\n","  loader = TextLoader(file_path)\n","  documents = loader.load()\n","  text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap= chunk_overlap)\n","  text  = text_splitter.split_documents(documents)\n","  return text\n","\n","\n","text = loadSplitDocuments(\"Basic Structure of the Local High Voltage Product _parsed.txt\", chunk_size = 600, chunk_overlap=60)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.2.1\n","Torch version: 2.2.1\n","Is CUDA enabled? True\n"]}],"source":["import torch\n","\n","print(torch.__version__)\n","\n","print(\"Torch version:\",torch.__version__)\n","\n","print(\"Is CUDA enabled?\",torch.cuda.is_available())"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T05:40:07.634314Z","iopub.status.busy":"2024-02-29T05:40:07.633856Z","iopub.status.idle":"2024-02-29T05:40:12.522144Z","shell.execute_reply":"2024-02-29T05:40:12.521334Z","shell.execute_reply.started":"2024-02-29T05:40:07.634274Z"},"trusted":true},"outputs":[],"source":["model_name = \"BAAI/bge-small-en\"\n","model_kwargs = {\"device\": \"cuda\"}\n","encode_kwargs = {\"normalize_embeddings\": True}\n","\n","bgeEmbeddings = HuggingFaceBgeEmbeddings(\n","                                     model_name=model_name,\n","                                     model_kwargs=model_kwargs,\n","                                     encode_kwargs=encode_kwargs)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T05:40:14.221658Z","iopub.status.busy":"2024-02-29T05:40:14.221050Z","iopub.status.idle":"2024-02-29T05:40:15.745187Z","shell.execute_reply":"2024-02-29T05:40:15.744399Z","shell.execute_reply.started":"2024-02-29T05:40:14.221611Z"},"trusted":true},"outputs":[],"source":["directory = 'db'\n","\n","vectordb = Chroma.from_documents(\n","                    documents = text,\n","                    embedding = bgeEmbeddings,\n","                    persist_directory= directory )\n","\n","vectordb.persist()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T05:40:15.746909Z","iopub.status.busy":"2024-02-29T05:40:15.746561Z","iopub.status.idle":"2024-02-29T05:40:15.820760Z","shell.execute_reply":"2024-02-29T05:40:15.819815Z","shell.execute_reply.started":"2024-02-29T05:40:15.746884Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[Document(page_content='Basic Structure of the Local High\\n\\ncase where  battery  relay  opens  then the inverter  output  off alert should  not be generated  since  we already\\nknow  that the output  has turned  off due to relay  opening\\n\\n//Cloud  Manager\\nThis is responsible  for uploading  all stats and alerts  to the cloud.  Usually  after every  2 minutes  the cloud  will\\nrequest  information  from  the system.\\n\\nWiFi Wizard\\nChecks  connection  of the system  with Wiﬁ.', metadata={'source': 'Basic Structure of the Local High Voltage Product _parsed.txt'}),\n"," Document(page_content='Basic Structure of the Local High\\n\\ncase where  battery  relay  opens  then the inverter  output  off alert should  not be generated  since  we already\\nknow  that the output  has turned  off due to relay  opening\\n\\n//Cloud  Manager\\nThis is responsible  for uploading  all stats and alerts  to the cloud.  Usually  after every  2 minutes  the cloud  will\\nrequest  information  from  the system.\\n\\nWiFi Wizard\\nChecks  connection  of the system  with Wiﬁ.', metadata={'source': 'Basic Structure of the Local High Voltage Product _parsed.txt'}),\n"," Document(page_content='Basic Structure of the Local High\\n\\ncase where  battery  relay  opens  then the inverter  output  off alert should  not be generated  since  we already\\nknow  that the output  has turned  off due to relay  opening\\n\\n//Cloud  Manager\\nThis is responsible  for uploading  all stats and alerts  to the cloud.  Usually  after every  2 minutes  the cloud  will\\nrequest  information  from  the system.\\n\\nWiFi Wizard\\nChecks  connection  of the system  with Wiﬁ.', metadata={'source': 'Basic Structure of the Local High Voltage Product _parsed.txt'}),\n"," Document(page_content='2. The Pi launches  the NGM  application\\n\\n3. SE Logger  Component  initialises\\n\\n4. SE LED Component  is initialised  and LEDs  start blinking  - this component  is ready  to\\nreceive  a message  from any other  component  upon  state  change\\n\\n5. Wifi wizard  launched  for connecting  system  to the Wifi\\n\\n6. DB Initialiser\\n\\n7. System registration to the Cloud - Blocking process: If the system is unable to register\\nto the Cloud then we cannot move forward - physically all the LEDs are still blinking,\\nhowever  the Cloud  LED will be in a state  of ‘wifi connected  but not yet Cloud  connected’', metadata={'source': 'Basic Structure of the Local High Voltage Product _parsed.txt'}),\n"," Document(page_content='2. The Pi launches  the NGM  application\\n\\n3. SE Logger  Component  initialises\\n\\n4. SE LED Component  is initialised  and LEDs  start blinking  - this component  is ready  to\\nreceive  a message  from any other  component  upon  state  change\\n\\n5. Wifi wizard  launched  for connecting  system  to the Wifi\\n\\n6. DB Initialiser\\n\\n7. System registration to the Cloud - Blocking process: If the system is unable to register\\nto the Cloud then we cannot move forward - physically all the LEDs are still blinking,\\nhowever  the Cloud  LED will be in a state  of ‘wifi connected  but not yet Cloud  connected’', metadata={'source': 'Basic Structure of the Local High Voltage Product _parsed.txt'})]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["directory = 'db'\n","vectordb = Chroma(persist_directory = directory, embedding_function= bgeEmbeddings)\n","\n","retriever = vectordb.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":5})\n","\n","docs = retriever.get_relevant_documents(\"When will the lab heating was started early morning at approximately?\")\n","docs"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T06:48:45.597320Z","iopub.status.busy":"2024-02-29T06:48:45.596958Z","iopub.status.idle":"2024-02-29T06:48:53.988488Z","shell.execute_reply":"2024-02-29T06:48:53.987695Z","shell.execute_reply.started":"2024-02-29T06:48:45.597295Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/torch/cuda/__init__.py:758: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025831440/work/c10/cuda/CUDAFunctions.cpp:108.)\n","  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n","You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"ename":"ImportError","evalue":"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer, LlamaForCausalLM, GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      4\u001b[0m Tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/wizardLM-7B-HF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTheBloke/wizardLM-7B-HF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/modeling_utils.py:3024\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3027\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3028\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n","\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"]}],"source":["import torch\n","from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n","\n","Tokenizer = LlamaTokenizer.from_pretrained(\"TheBloke/wizardLM-7B-HF\")\n","model = LlamaForCausalLM.from_pretrained(\"TheBloke/wizardLM-7B-HF\",\n","                                         load_in_4bit=True,\n","                                         torch_dtype=torch.float16)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from langchain.llms import HuggingFacePipeline\n","from transformers import pipeline\n","import torch\n","\n","pipe = pipeline(\n","               'text-generation',\n","                model=model,\n","                tokenizer=Tokenizer,\n","                max_length=1024,\n",")\n","\n","wizard_llm = HuggingFacePipeline(pipeline=pipe)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["recent_chats = \"\"\"\n","User: What is the purpose of the High Voltage Battery Management System (HVBMS)?\n","Assistant: The HVBMS is designed to monitor, balance, and protect LiFePO4 battery cells, ensuring they operate safely under various conditions.\n","\n","User: How many major portions does the HVBMS comprise, and what are they?\n","Assistant: The HVBMS comprises six major portions: one current sensing portion (CS), four cell balancing, monitoring, and protection portions (CBMPs), and one microcontroller portion (MCU).\n","\n","User: What communication interfaces does the HVBMS board support?\n","Assistant: The HVBMS board supports RS485 and CAN-Bus interfaces for communication with the\n","\"\"\""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from langchain.chains.question_answering import load_qa_chain\n","from langchain.memory import ConversationBufferMemory\n","\n","# memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"question\", return_messages=True)\n","\n","prompt_template = \"\"\"\n","                previous conversation:\n","                [{chat_history}]\n","                ---------------------\n","                \\nConsidering the above previous conversations and the given context, please answer the following question, taking into account conversation awareness.\n","                If you don't know the answer, simply state \"Contact NOC for further assistance\" without attempting to invent an answer.\n","                Context: \\n{context}\\n\\n\n","                Answer the question: \\n{question}\\n\n","                Answer:  \"\"\"\n","\n","prompt = PromptTemplate(template = prompt_template, input_variables = [\"chat_history\", \"context\", \"question\"])\n","chain = load_qa_chain(wizard_llm, chain_type=\"stuff\", prompt=prompt)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fd70bcf6-7c44-44d3-9435-567b82611e1c","showTitle":false,"title":""}},"source":["## Evaluate the RAG system using `mlflow.evaluate()`"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"de1bc359-2e40-459c-bea4-bed35a117988","showTitle":false,"title":""}},"source":["Create a simple function that runs each input through the RAG chain"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d1064306-b7f3-4b3e-825c-4353d808f21d","showTitle":false,"title":""}},"source":["Create an eval dataset"]},{"cell_type":"code","execution_count":50,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"a5481491-e4a9-42ea-8a3f-f527faffd04d","showTitle":false,"title":""},"execution":{"iopub.execute_input":"2024-02-29T06:39:29.623411Z","iopub.status.busy":"2024-02-29T06:39:29.622601Z","iopub.status.idle":"2024-02-29T06:39:29.629721Z","shell.execute_reply":"2024-02-29T06:39:29.628706Z","shell.execute_reply.started":"2024-02-29T06:39:29.623381Z"},"trusted":true},"outputs":[],"source":["eval_df = pd.DataFrame(\n","    {\n","    \"questions\": [\n","    \"Explore the interactions between the NGM app, BMS Manager, and Inverter Manager in ensuring efficient communication and data exchange across various system components, highlighting their roles in initialization and operational control.\",\n","    \"How do the Cloud Manager, WiFi Wizard, and Database Manager collaborate to maintain seamless connectivity and data synchronization between the local system and the cloud, considering their respective responsibilities and interactions?\",\n","    \"Discuss the collaborative efforts of the Bring Up Service, UI Manager, and Alerts Service in providing comprehensive system feedback to operators, considering their roles in initialization, user interface management, and alert generation.\",\n","    \"Explain how the Forecast Engine and Smart Flow Manager work together to optimize system performance based on forecasted data, emphasizing their roles in data processing and decision-making for backup and charge source strategies.\",\n","    \"Explore the interconnectedness of various components such as the NGM app, BMS Manager, and Inverter Manager in handling both routine operations and contingency scenarios, highlighting the importance of seamless coordination for overall system reliability.\",\n","    \"How do the NGM app, Cloud Manager, and Alerts Service contribute to system resilience in the face of network failures or hardware disconnectivity, considering their roles in system initialization, data transfer, and alert generation?\",\n","    \"Discuss the integration between the Database Manager and other components such as the Bring Up Service and UI Manager in maintaining consistent data access and user interface feedback, considering their roles in data management and system monitoring.\",\n","    \"Explain the collaborative efforts between the Inverter Manager, BMS Manager, and Smart Flow Manager in optimizing power flow and operational strategies based on real-time data and forecasted trends, emphasizing their roles in system stability and efficiency.\",\n","    \"How do the NGM app, WiFi Wizard, and Cloud Manager adapt to changing network conditions and ensure continuous system operation and data synchronization, considering their roles in network connectivity and cloud integration?\",\n","    \"Explore the overall system architecture and interactions among components such as the NGM app, Inverter Manager, and Alerts Service in ensuring reliable operation and timely response to various system events, highlighting the integration and coordination required for seamless functionality.\",\n","        ],\n","    }\n",")"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":112,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3882b940-9c25-41ce-a301-72d8c0c90aaa","showTitle":false,"title":""},"execution":{"iopub.execute_input":"2024-02-29T06:39:30.789574Z","iopub.status.busy":"2024-02-29T06:39:30.788890Z","iopub.status.idle":"2024-02-29T06:39:30.798889Z","shell.execute_reply":"2024-02-29T06:39:30.797909Z","shell.execute_reply.started":"2024-02-29T06:39:30.789546Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["EvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\n","Task:\n","You must return the following fields in your response in two lines, one below the other:\n","score: Your numerical score for the model's faithfulness based on the rubric\n","justification: Your reasoning about the model's faithfulness score\n","\n","You are an impartial judge. You will be given an input that was sent to a machine\n","learning model, and you will be given an output that the model produced. You\n","may also be given additional information that was used by the model to generate the output.\n","\n","Your task is to determine a numerical score called faithfulness based on the input and output.\n","A definition of faithfulness and a grading rubric are provided below.\n","You must use the grading rubric to determine your score. You must also justify your score.\n","\n","Examples could be included below for reference. Make sure to use them as references and to\n","understand them before completing the task.\n","\n","Input:\n","{input}\n","\n","Output:\n","{output}\n","\n","{grading_context_columns}\n","\n","Metric definition:\n","Faithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n","\n","Grading rubric:\n","Faithfulness: Below are the details for different scores:\n","- Score 1: None of the claims in the output can be inferred from the provided context.\n","- Score 2: Some of the claims in the output can be inferred from the provided context, but the majority of the output is missing from, inconsistent with, or contradictory to the provided context.\n","- Score 3: Half or more of the claims in the output can be inferred from the provided context.\n","- Score 4: Most of the claims in the output can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n","- Score 5: All of the claims in the output are directly supported by the provided context, demonstrating high faithfulness to the provided context.\n","\n","Examples:\n","\n","Example Output:\n","The NGM app facilitates system initialization by providing configurations to components and ensuring communication channels are established.\n","\n","Additional information used by the model:\n","key: context\n","value:\n","The NGM app is responsible for communicating with various components such as the BMS, Cloud, HV Control Board, and Inverter. It provides configurations for initial setup and ensures communication channels are established before the system starts functioning.\n","\n","Example score: 5\n","Example justification: The output accurately describes how the NGM app facilitates system initialization, matching the information provided in the context.\n","        \n","\n","Example Output:\n","The BMS Manager is responsible for handling all NGM based actions related to the battery and cloud. It communicates with all BMSes attached to the system, reads data such as pack current and voltage, and processes this data.\n","\n","Additional information used by the model:\n","key: context\n","value:\n","The BMS Manager serves as a crucial link between higher-level actions related to the battery and cloud interactions. It communicates with all BMSes attached to the system, reads data such as pack current and voltage, and processes this data before passing relevant information to other processes.\n","\n","Example score: 2\n","Example justification: While the output provides some information about the BMS Manager, it lacks detail and does not fully capture its role as described in the context.\n","        \n","\n","You must return the following fields in your response in two lines, one below the other:\n","score: Your numerical score for the model's faithfulness based on the rubric\n","justification: Your reasoning about the model's faithfulness score\n","\n","Do not add additional new lines. Do not add any other fields.\n","    )\n"]}],"source":["from mlflow.metrics.genai import EvaluationExample, faithfulness\n","\n","from dataclasses import dataclass\n","\n","@dataclass\n","class EvaluationExample:\n","    input: str\n","    output: str\n","    score: int\n","    justification: str\n","    grading_context: dict\n","\n","# Create a good and bad example for faithfulness in the context of this problem\n","faithfulness_examples = [\n","    EvaluationExample(\n","        input=\"How does the NGM app facilitate system initialization?\",\n","        output=\"The NGM app facilitates system initialization by providing configurations to components and ensuring communication channels are established.\",\n","        score=5,\n","        justification=\"The output accurately describes how the NGM app facilitates system initialization, matching the information provided in the context.\",\n","        grading_context={\n","            \"context\": \"The NGM app is responsible for communicating with various components such as the BMS, Cloud, HV Control Board, and Inverter. It provides configurations for initial setup and ensures communication channels are established before the system starts functioning.\"\n","        },\n","    ),\n","    EvaluationExample(\n","        input=\"What is the role of the BMS Manager?\",\n","        output=\"The BMS Manager is responsible for handling all NGM based actions related to the battery and cloud. It communicates with all BMSes attached to the system, reads data such as pack current and voltage, and processes this data.\",\n","        score=2,\n","        justification=\"While the output provides some information about the BMS Manager, it lacks detail and does not fully capture its role as described in the context.\",\n","        grading_context={\n","            \"context\": \"The BMS Manager serves as a crucial link between higher-level actions related to the battery and cloud interactions. It communicates with all BMSes attached to the system, reads data such as pack current and voltage, and processes this data before passing relevant information to other processes.\"\n","        },\n","    ),\n","]\n","\n","\n","faithfulness_metric = faithfulness(\n","    model=mpt_pipeline, examples=faithfulness_examples\n",")\n","print(faithfulness_metric)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T06:34:51.668822Z","iopub.status.busy":"2024-02-29T06:34:51.667802Z","iopub.status.idle":"2024-02-29T06:38:04.862381Z","shell.execute_reply":"2024-02-29T06:38:04.861406Z","shell.execute_reply.started":"2024-02-29T06:34:51.668779Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"]},{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 3/3 [00:26<00:00,  8.72s/it]\n"]}],"source":["model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","judge_tokenizer = AutoTokenizer.from_pretrained(model_name)\n","judge_model = AutoModelForCausalLM.from_pretrained(model_name,\n","                                         load_in_4bit=True,\n","                                         torch_dtype=torch.float16)"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T06:39:35.717910Z","iopub.status.busy":"2024-02-29T06:39:35.716969Z","iopub.status.idle":"2024-02-29T06:39:35.724242Z","shell.execute_reply":"2024-02-29T06:39:35.722997Z","shell.execute_reply.started":"2024-02-29T06:39:35.717865Z"},"trusted":true},"outputs":[],"source":["pipe = pipeline(\n","               'text-generation',\n","                model=judge_model,\n","                tokenizer=judge_tokenizer,\n","                max_length=2048,\n",")\n","\n","mistral_llm = HuggingFacePipeline(pipeline=pipe)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def model(input_df):\n","    answer = []\n","    for index, row in input_df.iterrows():\n","        answer.append(mistral_llm(row[\"questions\"]))\n","\n","    return answer"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]}],"source":["answers=model(eval_df)\n","eval_df[\"answers\"] = answers"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>questions</th>\n","      <th>answers</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Explore the interactions between the NGM app, ...</td>\n","      <td>\\n\\n## 1. Introduction\\n\\nThe integration of r...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>How do the Cloud Manager, WiFi Wizard, and Dat...</td>\n","      <td>\\n\\nThe Cloud Manager, WiFi Wizard, and Databa...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Discuss the collaborative efforts of the Bring...</td>\n","      <td>\\n\\nThe Bring Up Service, UI Manager, and Aler...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Explain how the Forecast Engine and Smart Flow...</td>\n","      <td>\\n\\nThe Forecast Engine and Smart Flow Manager...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Explore the interconnectedness of various comp...</td>\n","      <td>\\n\\n## 1. Introduction\\n\\nThe rapid developmen...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           questions  \\\n","0  Explore the interactions between the NGM app, ...   \n","1  How do the Cloud Manager, WiFi Wizard, and Dat...   \n","2  Discuss the collaborative efforts of the Bring...   \n","3  Explain how the Forecast Engine and Smart Flow...   \n","4  Explore the interconnectedness of various comp...   \n","\n","                                             answers  \n","0  \\n\\n## 1. Introduction\\n\\nThe integration of r...  \n","1  \\n\\nThe Cloud Manager, WiFi Wizard, and Databa...  \n","2  \\n\\nThe Bring Up Service, UI Manager, and Aler...  \n","3  \\n\\nThe Forecast Engine and Smart Flow Manager...  \n","4  \\n\\n## 1. Introduction\\n\\nThe rapid developmen...  "]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["eval_df.head()"]},{"cell_type":"code","execution_count":110,"metadata":{"execution":{"iopub.execute_input":"2024-02-29T07:51:06.486789Z","iopub.status.busy":"2024-02-29T07:51:06.486042Z","iopub.status.idle":"2024-02-29T07:51:06.493468Z","shell.execute_reply":"2024-02-29T07:51:06.492533Z","shell.execute_reply.started":"2024-02-29T07:51:06.486751Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["EvaluationMetric(name=relevance, greater_is_better=True, long_name=relevance, version=v1, metric_details=\n","Task:\n","You must return the following fields in your response in two lines, one below the other:\n","score: Your numerical score for the model's relevance based on the rubric\n","justification: Your reasoning about the model's relevance score\n","\n","You are an impartial judge. You will be given an input that was sent to a machine\n","learning model, and you will be given an output that the model produced. You\n","may also be given additional information that was used by the model to generate the output.\n","\n","Your task is to determine a numerical score called relevance based on the input and output.\n","A definition of relevance and a grading rubric are provided below.\n","You must use the grading rubric to determine your score. You must also justify your score.\n","\n","Examples could be included below for reference. Make sure to use them as references and to\n","understand them before completing the task.\n","\n","Input:\n","{input}\n","\n","Output:\n","{output}\n","\n","{grading_context_columns}\n","\n","Metric definition:\n","Relevance encompasses the appropriateness, significance, and applicability of the output with respect to both the input and context. Scores should reflect the extent to which the output directly addresses the question provided in the input, given the provided context.\n","\n","Grading rubric:\n","Relevance: Below are the details for different scores:- Score 1: The output doesn't mention anything about the question or is completely irrelevant to the provided context.\n","- Score 2: The output provides some relevance to the question and is somehow related to the provided context.\n","- Score 3: The output mostly answers the question and is largely consistent with the provided context.\n","- Score 4: The output answers the question and is consistent with the provided context.\n","- Score 5: The output answers the question comprehensively using the provided context.\n","\n","Examples:\n","\n","Example Input:\n","How is MLflow related to Databricks?\n","\n","Example Output:\n","Databricks is a data engineering and analytics platform designed to help organizations process and analyze large amounts of data. Databricks is a company specializing in big data and machine learning solutions.\n","\n","Additional information used by the model:\n","key: context\n","value:\n","MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n","\n","Example score: 2\n","Example justification: The output provides relevant information about Databricks, mentioning it as a company specializing in big data and machine learning solutions. However, it doesn't directly address how MLflow is related to Databricks, which is the specific question asked in the input. Therefore, the output is only somewhat related to the provided context.\n","        \n","\n","Example Input:\n","How is MLflow related to Databricks?\n","\n","Example Output:\n","MLflow is a product created by Databricks to enhance the efficiency of machine learning processes.\n","\n","Additional information used by the model:\n","key: context\n","value:\n","MLflow is an open-source platform for managing the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, a company that specializes in big data and machine learning solutions. MLflow is designed to address the challenges that data scientists and machine learning engineers face when developing, training, and deploying machine learning models.\n","\n","Example score: 4\n","Example justification: The output provides a relevant and accurate statement about the relationship between MLflow and Databricks. While it doesn't provide extensive detail, it still offers a substantial and meaningful response. To achieve a score of 5, the response could be further improved by providing additional context or details about how MLflow specifically functions within the Databricks ecosystem.\n","        \n","\n","You must return the following fields in your response in two lines, one below the other:\n","score: Your numerical score for the model's relevance based on the rubric\n","justification: Your reasoning about the model's relevance score\n","\n","Do not add additional new lines. Do not add any other fields.\n","    )\n","________________________________________________________________________\n","EvaluationMetric(name=faithfulness, greater_is_better=True, long_name=faithfulness, version=v1, metric_details=\n","Task:\n","You must return the following fields in your response in two lines, one below the other:\n","score: Your numerical score for the model's faithfulness based on the rubric\n","justification: Your reasoning about the model's faithfulness score\n","\n","You are an impartial judge. You will be given an input that was sent to a machine\n","learning model, and you will be given an output that the model produced. You\n","may also be given additional information that was used by the model to generate the output.\n","\n","Your task is to determine a numerical score called faithfulness based on the input and output.\n","A definition of faithfulness and a grading rubric are provided below.\n","You must use the grading rubric to determine your score. You must also justify your score.\n","\n","Examples could be included below for reference. Make sure to use them as references and to\n","understand them before completing the task.\n","\n","Input:\n","{input}\n","\n","Output:\n","{output}\n","\n","{grading_context_columns}\n","\n","Metric definition:\n","Faithfulness is only evaluated with the provided output and provided context, please ignore the provided input entirely when scoring faithfulness. Faithfulness assesses how much of the provided output is factually consistent with the provided context. A higher score indicates that a higher proportion of claims present in the output can be derived from the provided context. Faithfulness does not consider how much extra information from the context is not present in the output.\n","\n","Grading rubric:\n","Faithfulness: Below are the details for different scores:\n","- Score 1: None of the claims in the output can be inferred from the provided context.\n","- Score 2: Some of the claims in the output can be inferred from the provided context, but the majority of the output is missing from, inconsistent with, or contradictory to the provided context.\n","- Score 3: Half or more of the claims in the output can be inferred from the provided context.\n","- Score 4: Most of the claims in the output can be inferred from the provided context, with very little information that is not directly supported by the provided context.\n","- Score 5: All of the claims in the output are directly supported by the provided context, demonstrating high faithfulness to the provided context.\n","\n","Examples:\n","\n","Example Output:\n","The NGM app facilitates system initialization by providing configurations to components and ensuring communication channels are established.\n","\n","Additional information used by the model:\n","key: context\n","value:\n","The NGM app is responsible for communicating with various components such as the BMS, Cloud, HV Control Board, and Inverter. It provides configurations for initial setup and ensures communication channels are established before the system starts functioning.\n","\n","Example score: 5\n","Example justification: The output accurately describes how the NGM app facilitates system initialization, matching the information provided in the context.\n","        \n","\n","Example Output:\n","The BMS Manager is responsible for handling all NGM based actions related to the battery and cloud. It communicates with all BMSes attached to the system, reads data such as pack current and voltage, and processes this data.\n","\n","Additional information used by the model:\n","key: context\n","value:\n","The BMS Manager serves as a crucial link between higher-level actions related to the battery and cloud interactions. It communicates with all BMSes attached to the system, reads data such as pack current and voltage, and processes this data before passing relevant information to other processes.\n","\n","Example score: 2\n","Example justification: While the output provides some information about the BMS Manager, it lacks detail and does not fully capture its role as described in the context.\n","        \n","\n","You must return the following fields in your response in two lines, one below the other:\n","score: Your numerical score for the model's faithfulness based on the rubric\n","justification: Your reasoning about the model's faithfulness score\n","\n","Do not add additional new lines. Do not add any other fields.\n","    )\n"]}],"source":["from mlflow.metrics.genai import relevance\n","\n","relevance_metric = relevance(model=mpt_pipeline)\n","print(relevance_metric)\n","print(\"________________________________________________________________________\")\n","print(faithfulness_metric)"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.49s/it]\n"]}],"source":["mpt_pipeline = pipeline(\"text-generation\", model=\"TheBloke/wizardLM-7B-HF\", max_length=1000)"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_2928323/525965934.py:7: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.2``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n","  model_info = mlflow.transformers.log_model(\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/models/model.py:615: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.37.2``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n","  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n","2024/03/11 12:20:17 WARNING mlflow.transformers: Unable to find license information for this model. Please verify permissible usage for the model you are storing prior to use.\n","Registered model 'mpt-7b-chat' already exists. Creating a new version of this model...\n","Created version '6' of model 'mpt-7b-chat'.\n"]}],"source":["signature = mlflow.models.infer_signature(\n","    model_input=\"What are the three primary colors?\",\n","    model_output=\"The three primary colors are red, yellow, and blue.\",\n",")\n","\n","with mlflow.start_run():\n","    model_info = mlflow.transformers.log_model(\n","        transformers_model=mpt_pipeline,\n","        artifact_path=\"mpt-7b\",\n","        signature=signature,\n","        registered_model_name=\"mpt-7b-chat\",\n","    )"]},{"cell_type":"code","execution_count":118,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"ea40ce52-6ac7-4c20-9669-d24f80a6cebe","showTitle":false,"title":""},"execution":{"iopub.execute_input":"2024-02-29T08:35:15.244747Z","iopub.status.busy":"2024-02-29T08:35:15.243900Z","iopub.status.idle":"2024-02-29T08:35:15.392588Z","shell.execute_reply":"2024-02-29T08:35:15.391359Z","shell.execute_reply.started":"2024-02-29T08:35:15.244717Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 66/66 [00:46<00:00,  1.41it/s]\n","2024/03/11 12:21:59 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n","2024/03/11 12:21:59 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n","/home/tanzeel-abbas/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"ename":"ValueError","evalue":"Input length of input_ids is 43, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion-answering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevance_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatency\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol_mapping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_documents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(results\u001b[38;5;241m.\u001b[39mmetrics)\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/models/evaluation/base.py:2102\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data, model_type, targets, predictions, dataset_path, feature_names, evaluators, evaluator_config, custom_metrics, extra_metrics, custom_artifacts, validation_thresholds, baseline_model, env_manager, model_config, baseline_config, inference_params)\u001b[0m\n\u001b[1;32m   2099\u001b[0m predictions_expected_in_model_output \u001b[38;5;241m=\u001b[39m predictions \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2102\u001b[0m     evaluate_result \u001b[38;5;241m=\u001b[39m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_name_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions_expected_in_model_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, _ServedPyFuncModel):\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/models/evaluation/base.py:1252\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(model, model_type, dataset, run_id, evaluator_name_list, evaluator_name_to_conf_map, custom_metrics, extra_metrics, custom_artifacts, baseline_model, predictions)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m evaluator\u001b[38;5;241m.\u001b[39mcan_evaluate(model_type\u001b[38;5;241m=\u001b[39mmodel_type, evaluator_config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[1;32m   1251\u001b[0m         _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating the model with the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m evaluator.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m         eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m         eval_results\u001b[38;5;241m.\u001b[39mappend(eval_result)\n\u001b[1;32m   1266\u001b[0m _last_failed_evaluator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py:1962\u001b[0m, in \u001b[0;36mDefaultEvaluator.evaluate\u001b[0;34m(self, model_type, dataset, run_id, evaluator_config, model, custom_metrics, extra_metrics, custom_artifacts, baseline_model, predictions, **kwargs)\u001b[0m\n\u001b[1;32m   1960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m baseline_model:\n\u001b[1;32m   1961\u001b[0m         _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating candidate model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1962\u001b[0m     evaluation_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_baseline_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m baseline_model:\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluation_result\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py:1844\u001b[0m, in \u001b[0;36mDefaultEvaluator._evaluate\u001b[0;34m(self, model, is_baseline_model, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_metrics\u001b[38;5;241m.\u001b[39mremove(extra_metric)\n\u001b[1;32m   1843\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1844\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_model_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_latency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_latency\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_builtin_metrics_by_model_type()\n\u001b[1;32m   1847\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_pred)})\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py:1406\u001b[0m, in \u001b[0;36mDefaultEvaluator._generate_model_predictions\u001b[0;34m(self, compute_latency)\u001b[0m\n\u001b[1;32m   1403\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing model predictions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_latency:\n\u001b[0;32m-> 1406\u001b[0m     model_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_with_latency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1408\u001b[0m     model_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X_copy)\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py:1376\u001b[0m, in \u001b[0;36mDefaultEvaluator._generate_model_predictions.<locals>.predict_with_latency\u001b[0;34m(X_copy)\u001b[0m\n\u001b[1;32m   1374\u001b[0m single_input \u001b[38;5;241m=\u001b[39m row_data\u001b[38;5;241m.\u001b[39mto_frame()\u001b[38;5;241m.\u001b[39mT \u001b[38;5;28;01mif\u001b[39;00m is_dataframe \u001b[38;5;28;01melse\u001b[39;00m row_data\n\u001b[1;32m   1375\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1376\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1378\u001b[0m pred_latencies\u001b[38;5;241m.\u001b[39mappend(end_time \u001b[38;5;241m-\u001b[39m start_time)\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:506\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data, params)\u001b[0m\n\u001b[1;32m    503\u001b[0m params \u001b[38;5;241m=\u001b[39m _validate_params(params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m HAS_PYSPARK \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, SparkDataFrame):\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/transformers/__init__.py:1636\u001b[0m, in \u001b[0;36m_TransformersWrapper.predict\u001b[0;34m(self, data, params)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(entry, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m input_data):\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;66;03m# Validate each dict inside an input List[Dict]\u001b[39;00m\n\u001b[1;32m   1632\u001b[0m     \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m   1633\u001b[0m         _validate_input_dictionary_contains_only_strings_and_lists_of_strings(x)\n\u001b[1;32m   1634\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m input_data\n\u001b[1;32m   1635\u001b[0m     )\n\u001b[0;32m-> 1636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/transformers/__init__.py:1728\u001b[0m, in \u001b[0;36m_TransformersWrapper._predict\u001b[0;34m(self, data, model_config)\u001b[0m\n\u001b[1;32m   1725\u001b[0m         return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1726\u001b[0m         output_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_token_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1728\u001b[0m     raw_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_config_and_return_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# Handle the pipeline outputs\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supported_custom_generator_types \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline, transformers\u001b[38;5;241m.\u001b[39mTextGenerationPipeline\n\u001b[1;32m   1735\u001b[0m ):\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/mlflow/transformers/__init__.py:1554\u001b[0m, in \u001b[0;36m_TransformersWrapper._validate_model_config_and_return_output\u001b[0;34m(self, data, model_config, return_tensors)\u001b[0m\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1553\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_config)\n\u001b[0;32m-> 1554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:241\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1177\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1174\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1175\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:328\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/generation/utils.py:1466\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1461\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `generation_config` defines a `cache_implementation` that is not compatible with this model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1462\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Make sure it has a `_setup_cache` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1463\u001b[0m         )\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_cache(cache_cls, max_batch_size\u001b[38;5;241m=\u001b[39mbatch_size, max_cache_len\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m-> 1466\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_generation_mode(generation_config, assistant_model)\n","File \u001b[0;32m~/.conda/envs/ta_ml_env/lib/python3.10/site-packages/transformers/generation/utils.py:1186\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1185\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     )\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1196\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 43, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."]}],"source":["results = mlflow.evaluate(\n","    model_info.model_uri,\n","    eval_df,\n","    model_type=\"question-answering\",\n","    evaluators=\"default\",\n","    predictions=\"result\",\n","    extra_metrics=[faithfulness_metric, relevance_metric, mlflow.metrics.latency()],\n","    evaluator_config={\n","        \"col_mapping\": {\n","            \"inputs\": \"questions\",\n","            \"context\": \"source_documents\",\n","        }\n","    },\n",")\n","print(results.metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"989a0861-5153-44e6-a19d-efcae7fe6cb5","showTitle":false,"title":""},"execution":{"iopub.execute_input":"2024-02-29T07:31:17.605470Z","iopub.status.busy":"2024-02-29T07:31:17.604737Z","iopub.status.idle":"2024-02-29T07:31:17.657943Z","shell.execute_reply":"2024-02-29T07:31:17.656687Z","shell.execute_reply.started":"2024-02-29T07:31:17.605437Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'results' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mtables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_results_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"]}],"source":["results.tables[\"eval_results_table\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"LLM Evaluation Examples -- RAG","widgets":{}},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4507148,"sourceId":7717206,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.1.0"}},"nbformat":4,"nbformat_minor":4}
